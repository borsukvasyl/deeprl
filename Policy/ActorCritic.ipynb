{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Network\n",
    "\n",
    "Both value and policy based methods have big drawbacks. That's why we use \"hybrid method\" Actor Critic, which has two networks:\n",
    "- a Critic which measures how good the taken action is\n",
    "- an Actor that controls how our agent behaves\n",
    "\n",
    "The Policy Gradient method has a big problem because of Monte Carlo, which waits until the end of episode to calculate the reward. We may conclude that if we have a high reward $R(t)$, all actions that we took were good, even if some were really bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic\n",
    "\n",
    "Instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step (TD Learning).\n",
    "\n",
    "Because we do an update at each time step, we can't use the total rewards $R(t)$. Instead, we need to train a Critic model that approximates the Q-value function. This value function replaces the reward function in policy gradient that calculates the rewards only at the end of the episode.\n",
    "\n",
    "Because we have two models (Actor and Critic) that must be trained, it means that we have two set of weights ($\\theta$ for our action and $w$ for our Critic) that must be optimized separately:\n",
    "$$\\Delta \\theta = \\alpha_1 \\nabla_{\\theta}(\\log \\pi_{\\theta}(s, a)) q_{w}(s, a)$$\n",
    "$$\\Delta w = \\alpha_2 \\nabla_{w} L(R(s, a) + \\lambda q_{w}(s_{t + 1}, a_{t + 1}), q_{w}(s_t - a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor Critic\n",
    "\n",
    "Value-based methods have high variability. To reduce this problem we use advantage function instead of value function:\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "where $V(s)$ is average value of that state. This function will tell us the improvement compared to the average the action taken at that state is.\n",
    "\n",
    "The problem of implementing this advantage function is that is requires two value functions - $Q(s,a)$ and $V(s)$. Fortunately, we can use the TD error as a good estimator of the advantage function:\n",
    "$$A(s, a) = Q(s, a) - V(s) = r + \\lambda V(s') - V(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36rl)",
   "language": "python",
   "name": "py36rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
