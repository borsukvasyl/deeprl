{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Action__ (A): A is the set of all possible moves the agent can make.\n",
    "- __State__ (S): A state is a concrete and immediate situation in which the agent finds itself; i.e. a specific place and moment.\n",
    "- __Reward__ (R): A reward is the feedback by which we measure the success or failure of an agentâ€™s actions.\n",
    "- __Policy__ ($\\pi$): The policy is the strategy that the agent employs to determine the next action based on the current state. It maps states to actions, the actions that promise the highest reward.\n",
    "\n",
    "Our task in reinforcement learning is to maximize the __cumulative reward__:\n",
    "$$G_t = R_{t + 1} + R_{t + 2} + R_{t + 3} + \\ldots = \\sum_{k = 0}^{T} R_{t + k + 1}$$\n",
    "However, in reality, we can't just add the rewards like that. The rewards that come sooner (in the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.<br>\n",
    "Our __discounted cumulative rewards__ is:\n",
    "$$G_t = R_{t + 1} + \\lambda R_{t + 2} + \\lambda^{2} R_{t + 3} + \\ldots = \\sum_{k = 0}^{T} \\lambda^k R_{t + k + 1}$$\n",
    "where $\\lambda \\in [0, 1)$<br>\n",
    "We have three approaches to solve Reinforcement Learning problem:\n",
    "1. The __value__ of each state is the total amount of the reward an agent can expect to accumulate over the future, starting at that state:\n",
    "   $$v_{\\pi} = \\mathbb{E} [R_{t + 1} + \\lambda R_{t + 2} + \\lambda^{2} R_{t + 3} + \\ldots \\mid S_t = s]$$\n",
    "   The agent will use this value function to select which state to choose at each step. The agent takes the state with the biggest value.\n",
    "   \n",
    "2. In __policy-based__ RL, we want to directly optimize the policy function $\\pi(s)$ without using a value function.\n",
    "\n",
    "3. In __model-based__ RL, we model the environment. This means we create a model of the behavior of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36rl)",
   "language": "python",
   "name": "py36rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
