{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "The __Action Value Function__ (or __\"Q-function\"__) takes two inputs: \"state\" and \"action\". It returns the expected future reward of that action at that state:\n",
    "$$Q^{\\pi}_{s_t, a_t} = \\mathbb{E} [R_{t + 1} + \\lambda R_{t + 2} + \\lambda^{2} R_{t + 3} + \\ldots \\mid s_t, a_t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-table\n",
    "We will use Q-table (\"Q\" for \"quality\" of the action), which stores Q-values for all states and actions. The columns will be the actions. The rows will be the states. The value of each cell will be the maximum expected future reward for that given state and action.<br>\n",
    "To find Q(s,a) we use the Bellman equation, which updates our Q-value based on new observations:\n",
    "$$NewQ(s, a) = Q(s, a) + \\alpha(R(s, a) + \\lambda \\max Q'(s', a') - Q(s, a))$$\n",
    "where $s'$ and $\\alpha$ are next state and learning rate respectively.\n",
    "\n",
    "Algorithm:\n",
    "```\n",
    "Initialize Q-table with size n(number of states) x m(number of actions) with 0 values\n",
    "for life or until learning is stopped:\n",
    "    Choose an action (a) in the current state (s) based on current Q-value estimates (argmax(Q(s, :)))\n",
    "    Take an action (a) and observe the outcome state (s') and reward (r)\n",
    "    Update Q(s, a) using Bellman equation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "But if we have a lot of states it is not efficient to create Q-table, so we can use neural network to predict Q values for all actions using given state.<br>\n",
    "In this case we have to minimize difference between our predicted Q-value and target Q-value, which is equal to $R(s, a) + \\lambda \\max_a Q(s', a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:55:25.358435Z",
     "start_time": "2018-11-06T12:55:23.702557Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:55:25.384763Z",
     "start_time": "2018-11-06T12:55:25.361243Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 40\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:55:25.424330Z",
     "start_time": "2018-11-06T12:55:25.388360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 2\n",
      "State space size: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "a_size = env.action_space.n\n",
    "s_size = env.observation_space.shape[0]\n",
    "print(\"Action space size: {}\".format(a_size))\n",
    "print(\"State space size: {}\".format(s_size))\n",
    "\n",
    "possible_actions = np.identity(a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:55:25.438027Z",
     "start_time": "2018-11-06T12:55:25.429054Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork(object):\n",
    "    def __init__(self, s_size, a_size):\n",
    "        self.s_size = s_size\n",
    "        self.a_size = a_size\n",
    "        \n",
    "        self.states = tf.placeholder(shape=[None, self.s_size], dtype=tf.float32)\n",
    "        self.dense = tf.layers.dense(inputs=self.states, units=20, activation=tf.nn.tanh)\n",
    "        self.Qout = tf.layers.dense(inputs=self.dense, units=self.a_size)\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "        self.Qtarget = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None ,self.a_size], dtype=tf.float32)\n",
    "        Q = tf.reduce_sum(tf.multiply(self.Qout, self.action), axis=1)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.Qtarget - Q))\n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        self.optimize = trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:57:11.254401Z",
     "start_time": "2018-11-06T12:57:11.057106Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "network = DQNetwork(s_size, a_size)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:57:15.114168Z",
     "start_time": "2018-11-06T12:57:15.080104Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:57:38.615457Z",
     "start_time": "2018-11-06T12:57:19.389178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPIDOSE 00000: nan\n",
      "EPIDOSE 00010: 25.444444444444443\n",
      "EPIDOSE 00020: 24.333333333333332\n",
      "EPIDOSE 00030: 31.444444444444443\n",
      "EPIDOSE 00040: 31.22222222222222\n",
      "EPIDOSE 00050: 37.111111111111114\n",
      "EPIDOSE 00060: 54.888888888888886\n",
      "EPIDOSE 00070: 41.111111111111114\n",
      "EPIDOSE 00080: 51.888888888888886\n",
      "EPIDOSE 00090: 77.77777777777777\n",
      "EPIDOSE 00100: 61.111111111111114\n",
      "EPIDOSE 00110: 82.77777777777777\n",
      "EPIDOSE 00120: 95.77777777777777\n",
      "EPIDOSE 00130: 49.111111111111114\n",
      "EPIDOSE 00140: 79.77777777777777\n",
      "EPIDOSE 00150: 59.22222222222222\n",
      "EPIDOSE 00160: 65.0\n",
      "EPIDOSE 00170: 85.88888888888889\n",
      "EPIDOSE 00180: 57.111111111111114\n",
      "EPIDOSE 00190: 112.44444444444444\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "n_steps = 2000\n",
    "e = 1\n",
    "e_decay = 0.995\n",
    "e_min = 0.1\n",
    "num_episodes = 200\n",
    "batch_size = 40\n",
    "\n",
    "rlist = []\n",
    "experience = deque(maxlen=2000)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    r_total = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.rand(1) < e:\n",
    "            a_ind = env.action_space.sample()\n",
    "        else:\n",
    "            a_ind = sess.run(network.predict, feed_dict={network.states: [s]})[0]\n",
    "        s1, r, done, _ = env.step(a_ind)\n",
    "        \n",
    "        experience.append((s, possible_actions[a_ind], r, s1, done))\n",
    "        \n",
    "        r_total += r\n",
    "        s = s1\n",
    "        if done:\n",
    "            if e > e_min:\n",
    "                e *= e_decay\n",
    "            if episode % 10 == 0:\n",
    "                print(\"EPIDOSE {:0>5}: {}\".format(episode, np.mean(rlist[-10:-1])))\n",
    "        \n",
    "        if len(experience) > batch_size:\n",
    "            minibatch = random.sample(experience, batch_size)\n",
    "            states_mb = np.array([i[0] for i in minibatch])\n",
    "            actions_mb = np.array([i[1] for i in minibatch])\n",
    "            rewards_mb = np.array([i[2] for i in minibatch])\n",
    "            next_states_mb = np.array([i[3] for i in minibatch])\n",
    "            dones_mb = np.array([i[4] for i in minibatch])\n",
    "            \n",
    "            Qtarget = []\n",
    "            Qnext_state = sess.run(network.Qout, feed_dict={network.states: next_states_mb})\n",
    "            for i in range(batch_size):\n",
    "                target = rewards_mb[i]\n",
    "                if not dones_mb[i]:\n",
    "                    target += gamma * np.max(Qnext_state[i])\n",
    "                Qtarget.append(target)\n",
    "            loss, _ = sess.run([network.loss, network.optimize], feed_dict={network.states: states_mb,\n",
    "                                                                           network.Qtarget: Qtarget,\n",
    "                                                                           network.action: actions_mb})\n",
    "\n",
    "    rlist.append(r_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:58:26.555057Z",
     "start_time": "2018-11-06T12:58:23.329154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for i in range(3): env.step(1)\n",
    "r_total = 0\n",
    "done = False\n",
    "while True:\n",
    "    env.render()\n",
    "    a = sess.run(network.predict, feed_dict={network.states: [s]})[0]\n",
    "    s, r, done, _ = env.step(a)\n",
    "    r_total += r\n",
    "#     print(done)\n",
    "    if done == True:\n",
    "        print(r_total)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T12:58:30.414954Z",
     "start_time": "2018-11-06T12:58:30.407016Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36rl)",
   "language": "python",
   "name": "py36rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
