{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Network\n",
    "\n",
    "Both value and policy based methods have big drawbacks. That's why we use \"hybrid method\" Actor Critic, which has two networks:\n",
    "- a Critic which measures how good the taken action is\n",
    "- an Actor that controls how our agent behaves\n",
    "\n",
    "The Policy Gradient method has a big problem because of Monte Carlo, which waits until the end of episode to calculate the reward. We may conclude that if we have a high reward $R(t)$, all actions that we took were good, even if some were really bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic\n",
    "\n",
    "Instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step (TD Learning).\n",
    "\n",
    "Because we do an update at each time step, we can't use the total rewards $R(t)$. Instead, we need to train a Critic model that approximates the Q-value function. This value function replaces the reward function in policy gradient that calculates the rewards only at the end of the episode.\n",
    "\n",
    "Because we have two models (Actor and Critic) that must be trained, it means that we have two set of weights ($\\theta$ for our action and $w$ for our Critic) that must be optimized separately:\n",
    "$$\\Delta \\theta = \\alpha_1 \\nabla_{\\theta}(\\log \\pi_{\\theta}(s, a)) q_{w}(s, a)$$\n",
    "$$\\Delta w = \\alpha_2 \\nabla_{w} L(R(s, a) + \\lambda q_{w}(s_{t + 1}, a_{t + 1}), q_{w}(s_t, a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor Critic\n",
    "\n",
    "Value-based methods have high variability. To reduce this problem we use advantage function instead of value function:\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "where $V(s)$ is average value of that state. This function will tell us the improvement compared to the average the action taken at that state is.\n",
    "\n",
    "The problem of implementing this advantage function is that is requires two value functions  -  $Q(s,a)$ and $V(s)$. Fortunately, we can use the TD error as a good estimator of the advantage function:\n",
    "$$A(s, a) = Q(s, a) - V(s) = r + \\lambda V(s') - V(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:24.138989Z",
     "start_time": "2018-11-06T13:00:23.144031Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:24.174103Z",
     "start_time": "2018-11-06T13:00:24.159809Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 40\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:24.214587Z",
     "start_time": "2018-11-06T13:00:24.193972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 2\n",
      "State space size: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "a_size = env.action_space.n\n",
    "s_size = env.observation_space.shape[0]\n",
    "\n",
    "print(\"Action space size: {}\".format(a_size))\n",
    "print(\"State space size: {}\".format(s_size))\n",
    "\n",
    "possible_actions = np.identity(a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:24.246614Z",
     "start_time": "2018-11-06T13:00:24.229074Z"
    }
   },
   "outputs": [],
   "source": [
    "class A2CNetwork(object):\n",
    "    def __init__(self, s_size, a_size, learning_rate=0.01):\n",
    "        self.a_size = a_size\n",
    "        self.s_size = s_size\n",
    "        \n",
    "        self.states = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        self.dense = tf.layers.dense(inputs=self.states, units=32, activation=tf.nn.relu)\n",
    "        self.policy = tf.layers.dense(inputs=self.dense, units=self.a_size, activation=tf.nn.softmax)\n",
    "        self.value = tf.layers.dense(inputs=self.dense, units=1)\n",
    "        \n",
    "        self.actions = tf.placeholder(shape=[None, a_size], dtype=tf.float32)\n",
    "        self.target_values = tf.placeholder(shape=[None,], dtype=tf.float32)\n",
    "        self.advantages = tf.placeholder(shape=[None,], dtype=tf.float32)\n",
    "        \n",
    "        # policy loss\n",
    "        log_prob = tf.log(tf.clip_by_value(self.policy, 0.000001, 0.999999))\n",
    "        neg_log_responsible_policy = -tf.reduce_sum(tf.multiply(log_prob, self.actions), reduction_indices=1)\n",
    "        self.policy_loss = tf.reduce_mean(tf.multiply(neg_log_responsible_policy, self.advantages))\n",
    "        \n",
    "        # value loss\n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.target_values - self.value))\n",
    "        \n",
    "        #loss\n",
    "        self.loss = 0.5 * self.value_loss + self.policy_loss\n",
    "        \n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.optimize = trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:24.617861Z",
     "start_time": "2018-11-06T13:00:24.278161Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "network = A2CNetwork(s_size, a_size)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:24.685618Z",
     "start_time": "2018-11-06T13:00:24.637620Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:36.919996Z",
     "start_time": "2018-11-06T13:00:24.712093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPIDOSE 00000: 33.0\n",
      "EPIDOSE 00010: 21.0\n",
      "EPIDOSE 00020: 15.0\n",
      "EPIDOSE 00030: 17.0\n",
      "EPIDOSE 00040: 45.0\n",
      "EPIDOSE 00050: 19.0\n",
      "EPIDOSE 00060: 26.0\n",
      "EPIDOSE 00070: 53.0\n",
      "EPIDOSE 00080: 40.0\n",
      "EPIDOSE 00090: 37.0\n",
      "EPIDOSE 00100: 127.0\n",
      "EPIDOSE 00110: 37.0\n",
      "EPIDOSE 00120: 88.0\n",
      "EPIDOSE 00130: 88.0\n",
      "EPIDOSE 00140: 45.0\n",
      "EPIDOSE 00150: 32.0\n",
      "EPIDOSE 00160: 106.0\n",
      "EPIDOSE 00170: 200.0\n",
      "EPIDOSE 00180: 154.0\n",
      "EPIDOSE 00190: 108.0\n",
      "EPIDOSE 00200: 150.0\n",
      "EPIDOSE 00210: 19.0\n",
      "EPIDOSE 00220: 101.0\n",
      "EPIDOSE 00230: 180.0\n",
      "EPIDOSE 00240: 27.0\n",
      "EPIDOSE 00250: 33.0\n",
      "EPIDOSE 00260: 96.0\n",
      "EPIDOSE 00270: 200.0\n",
      "EPIDOSE 00280: 81.0\n",
      "EPIDOSE 00290: 26.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "min_batch_size = 32\n",
    "discount_factor = 0.95\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    episode_states = []\n",
    "    episode_rewards = []\n",
    "    episode_actions = []\n",
    "    episode_values = []\n",
    "    r_total = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        pi, value = sess.run([network.policy, network.value], feed_dict={\n",
    "            network.states: [s]\n",
    "        })\n",
    "        action = np.random.choice(a_size, p=pi[0])\n",
    "        s1, r, done, _ = env.step(action)\n",
    "        \n",
    "        action_vec = possible_actions[action]\n",
    "        \n",
    "        episode_states.append(s)\n",
    "        episode_rewards.append(r)\n",
    "        episode_actions.append(action_vec)\n",
    "        episode_values.append(value[0][0])\n",
    "        r_total += r\n",
    "        \n",
    "        if done or len(episode_states) > min_batch_size:\n",
    "            target_value = 0\n",
    "            if not done: \n",
    "                target_value = sess.run(network.value, feed_dict={network.states: [s1]})[0]\n",
    "\n",
    "            target_values = np.zeros_like(episode_rewards)\n",
    "            for i in range(len(episode_states) - 1, -1, -1):\n",
    "                target_value = episode_rewards[i] +  discount_factor * target_value\n",
    "                target_values[i] = target_value\n",
    "\n",
    "            advantages = target_values - np.array(episode_values)\n",
    "\n",
    "            loss, _ = sess.run([network.loss, network.optimize], feed_dict={\n",
    "                network.states: episode_states,\n",
    "                network.advantages: advantages,\n",
    "                network.actions: episode_actions,\n",
    "                network.target_values: target_values\n",
    "            })\n",
    "\n",
    "\n",
    "            episode_states = []\n",
    "            episode_rewards = []\n",
    "            episode_actions = []\n",
    "            episode_values = []\n",
    "        \n",
    "        if done and episode % 10 == 0:\n",
    "            print(\"EPIDOSE {:0>5}: {}\".format(episode, r_total))\n",
    "        \n",
    "        s = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-06T13:02:03.543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "# for i in range(3): env.step(0)\n",
    "r_total = 0\n",
    "done = False\n",
    "while True:\n",
    "    env.render()\n",
    "    pi = sess.run(network.policy, feed_dict={network.states: [s]})\n",
    "    a = np.random.choice(a_size, p=pi[0])\n",
    "    s, r, done, _ = env.step(a)\n",
    "    r_total += r\n",
    "    #print(done)\n",
    "#     if done == True:\n",
    "#         print(r_total)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:00:40.629775Z",
     "start_time": "2018-11-06T13:00:40.612581Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36rl)",
   "language": "python",
   "name": "py36rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
