{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In policy-based methods, instead of learning a value function that tells us what is the expected sum of rewards given a state and an action, we learn directly the policy function that maps state to action (select actions without using a value function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Policy Gradient over Deep Q-Learning:\n",
    "- __Convergence__: \n",
    "  \n",
    "  Policy-based methods have better convergence properties. The problem with value-based methods is that they can have a big oscillation while training. This is because the choice of action may change dramatically for an arbitrarily small change in the estimated action values. On the other hand, with policy gradient, we just follow the gradient to find the best parameters. We see a smooth update of our policy at each step.\n",
    "\n",
    "\n",
    "- __Policy gradients are more effective in high dimensional action spaces or when using continuous actions__:\n",
    "  \n",
    "  The problem with Deep Q-learning is that their predictions assign a score (maximum expected future reward) for each possible action, at each time step, given the current state. On the other hand, in policy-based methods, you just adjust the parameters directly: thanks to that you’ll start to understand what the maximum will be, rather than computing (estimating) the maximum directly at every step.\n",
    "\n",
    "\n",
    "- __Policy gradients can learn stochastic policies__:\n",
    "  \n",
    "  One of these is that we don’t need to implement an exploration/exploitation trade off. A stochastic policy allows our agent to explore the state space without always taking the same action. This is because it outputs a probability distribution over actions. \n",
    "  \n",
    "  We also get rid of the problem of perceptual aliasing. Perceptual aliasing is when we have two states that seem to be (or actually are) the same, but need different actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "Naturally, Policy gradients have one big disadvantage. A lot of the time, they converge on a local maximum rather than on the global optimum.\n",
    "\n",
    "Instead of Deep Q-Learning, which always tries to reach the maximum, policy gradients converge slower, step by step. They can take longer to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search\n",
    "\n",
    "We have to maximize score function $J(\\theta) = \\mathbb{E}_{\\pi}(\\sum \\lambda r)$. The main idea here is that $J(\\theta)$ will tell us how good our $\\pi$ is. Policy gradient ascent will help us to find the best policy parameters to maximize the sample of good actions.\n",
    "\n",
    "Our score can also be defined as:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi} (R(\\tau)) = \\sum_{\\tau} \\pi(\\tau \\mid \\theta) R(\\tau)$$\n",
    "where $\\tau$ is route.\n",
    "\n",
    "Then we can calculate gradient:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} J(\\theta) &= \\nabla_{\\theta} \\sum_{\\tau} \\pi(\\tau \\mid \\theta) R(\\tau)\\\\\n",
    "&= \\sum_{\\tau} \\nabla_{\\theta} \\pi(\\tau \\mid \\theta) R(\\tau)\\\\\n",
    "&= \\sum_{\\tau} \\pi(\\tau \\mid \\theta) \\frac{\\nabla_{\\theta} \\pi(\\tau \\mid \\theta)}{\\pi(\\tau \\mid \\theta)} R(\\tau)\\\\\n",
    "&= \\sum_{\\tau} \\pi(\\tau \\mid \\theta) \\nabla_{\\theta} (\\log \\pi(\\tau \\mid \\theta)) R(\\tau)\\\\\n",
    "&= \\mathbb{E}_{\\pi} [\\nabla_{\\theta} (\\log \\pi(\\tau \\mid \\theta)) R(\\tau)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So that our update rule is: \n",
    "$$\\Delta \\theta = \\alpha \\nabla_{\\theta} (\\log \\pi(\\tau \\mid \\theta)) R(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Gradient\n",
    "\n",
    "```\n",
    "Initialize θ\n",
    "for each episode τ = S0, A0, R1, S1, …, ST:\n",
    "    for t <-- 1 to T-1:\n",
    "        Δθ = α ∇theta(log π(St, At, θ)) Gt\n",
    "        θ = θ + Δθ\n",
    "```\n",
    "But we face a problem with this algorithm. Because we only calculate R at the end of the episode, we average all actions. Even if some of the actions taken were very bad, if our score is quite high, we will average all the actions as good.\n",
    "\n",
    "This can be fixed using Actor Critic (a hybrid between value-based algorithms and policy-based algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:19.321263Z",
     "start_time": "2018-11-06T13:04:19.309097Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:19.670310Z",
     "start_time": "2018-11-06T13:04:19.656829Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 40\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:19.824180Z",
     "start_time": "2018-11-06T13:04:19.818287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 2\n",
      "State space size: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "a_size = env.action_space.n\n",
    "s_size = env.observation_space.shape[0]\n",
    "print(\"Action space size: {}\".format(a_size))\n",
    "print(\"State space size: {}\".format(s_size))\n",
    "\n",
    "possible_actions = np.identity(a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:20.019760Z",
     "start_time": "2018-11-06T13:04:20.014130Z"
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(episode_rewards, gamma=0.95):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0\n",
    "    for i in range(len(episode_rewards) - 1, -1, -1):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:20.177128Z",
     "start_time": "2018-11-06T13:04:20.163142Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "    def __init__(self, s_size, a_size, learning_rate=0.01):\n",
    "        self.s_size = s_size\n",
    "        self.a_size = a_size\n",
    "        \n",
    "        self.states = tf.placeholder(shape=[None, self.s_size], dtype=tf.float32)\n",
    "        self.dense = tf.layers.dense(inputs=self.states, units=32, activation=tf.nn.relu)\n",
    "        self.policy = tf.layers.dense(inputs=self.dense, units=self.a_size, activation=tf.nn.softmax)\n",
    "        \n",
    "        self.actions = tf.placeholder(shape=[None, self.a_size], dtype=tf.float32)\n",
    "        self.discounted_episode_rewards = tf.placeholder(shape=[None,], dtype=tf.float32)\n",
    "        \n",
    "        log_prob = tf.log(tf.clip_by_value(self.policy, 0.000001, 0.999999))\n",
    "        neg_log_responsible_policy = -tf.reduce_sum(tf.multiply(log_prob, self.actions), reduction_indices=1)\n",
    "        self.loss = tf.reduce_mean(neg_log_responsible_policy * self.discounted_episode_rewards)\n",
    "        \n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.optimize = trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:20.865178Z",
     "start_time": "2018-11-06T13:04:20.553540Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "network = PolicyNetwork(s_size, a_size)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:21.130057Z",
     "start_time": "2018-11-06T13:04:21.099647Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:33.657631Z",
     "start_time": "2018-11-06T13:04:22.013332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPIDOSE 00000: 34.0\n",
      "EPIDOSE 00010: 27.0\n",
      "EPIDOSE 00020: 43.0\n",
      "EPIDOSE 00030: 89.0\n",
      "EPIDOSE 00040: 31.0\n",
      "EPIDOSE 00050: 14.0\n",
      "EPIDOSE 00060: 18.0\n",
      "EPIDOSE 00070: 18.0\n",
      "EPIDOSE 00080: 31.0\n",
      "EPIDOSE 00090: 14.0\n",
      "EPIDOSE 00100: 25.0\n",
      "EPIDOSE 00110: 20.0\n",
      "EPIDOSE 00120: 43.0\n",
      "EPIDOSE 00130: 13.0\n",
      "EPIDOSE 00140: 36.0\n",
      "EPIDOSE 00150: 58.0\n",
      "EPIDOSE 00160: 30.0\n",
      "EPIDOSE 00170: 50.0\n",
      "EPIDOSE 00180: 49.0\n",
      "EPIDOSE 00190: 28.0\n",
      "EPIDOSE 00200: 22.0\n",
      "EPIDOSE 00210: 32.0\n",
      "EPIDOSE 00220: 73.0\n",
      "EPIDOSE 00230: 93.0\n",
      "EPIDOSE 00240: 127.0\n",
      "EPIDOSE 00250: 120.0\n",
      "EPIDOSE 00260: 175.0\n",
      "EPIDOSE 00270: 200.0\n",
      "EPIDOSE 00280: 200.0\n",
      "EPIDOSE 00290: 200.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    r_total = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        pi = sess.run(network.policy, feed_dict={network.states: [s]})\n",
    "        action = np.random.choice(a_size, p=pi[0])\n",
    "        s1, r, done, _ = env.step(action)\n",
    "        \n",
    "        action_vec = np.zeros(a_size)\n",
    "        action_vec[action] = 1\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(action_vec)\n",
    "        rewards.append(r)\n",
    "        r_total += r\n",
    "        \n",
    "        if done:\n",
    "            discounted_rewards = discount_rewards(rewards)\n",
    "            loss, _ = sess.run([network.loss, network.optimize],\n",
    "                               feed_dict={network.states: states,\n",
    "                                          network.actions: actions,\n",
    "                                          network.discounted_episode_rewards: discounted_rewards})\n",
    "            if episode % 10 == 0:\n",
    "                print(\"EPIDOSE {:0>5}: {}\".format(episode, r_total))\n",
    "        \n",
    "        s = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:37.057920Z",
     "start_time": "2018-11-06T13:04:33.739952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "# for i in range(3): env.step(0)\n",
    "r_total = 0\n",
    "done = False\n",
    "while True:\n",
    "    env.render()\n",
    "    pi = sess.run(network.policy, feed_dict={network.states: [s]})\n",
    "    a = np.random.choice(a_size, p=pi[0])\n",
    "    s, r, done, _ = env.step(a)\n",
    "    r_total += r\n",
    "    #print(done)\n",
    "    if done == True:\n",
    "        print(r_total)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T13:04:39.543761Z",
     "start_time": "2018-11-06T13:04:39.513870Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36rl)",
   "language": "python",
   "name": "py36rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
